{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.0.74\n",
      "azure-ml-test\teastus2\tazureml\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment = Experiment(workspace=ws, name=\"FakeNewsDetect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply for compute target resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target: gpu-cluster\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 1)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "        min_nodes = compute_min_nodes, \n",
    "        max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit task script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"ml_scripts\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/azmnt/code/Users/wck593462925/ml_scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# 安装依赖包\n",
    "os.system(\"pip install transformers\")\n",
    "os.system(\"pip install pandas\")\n",
    "os.system('pip install \"azureml-dataprep[fuse,pandas]\"')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# 创建实验并登录数据库\n",
    "ws = Workspace(subscription_id=\"6e12bb22-e64d-4e66-b7ad-a5ef94af1799\", resource_group=\"AzureML\", workspace_name=\"azure-ml-test\")\n",
    "experiment = Experiment(workspace=ws, name=\"FakeNewsDetectSmall\")\n",
    "datastore = Datastore.get(ws, datastore_name='workspaceblobstore')\n",
    "\n",
    "# 提取训练集数据\n",
    "train_set_name = \"FakeNewsTrain\"\n",
    "train_set = azureml.core.Dataset.get_by_name(workspace=ws, name=train_set_name)\n",
    "df_train = train_set.to_pandas_dataframe()\n",
    "empty_title = ((df_train['title2_zh'].isnull()) \\\n",
    "               | (df_train['title1_zh'].isnull()) \\\n",
    "               | (df_train['title2_zh'] == '') \\\n",
    "               | (df_train['title2_zh'] == '0'))\n",
    "df_train = df_train[~empty_title]\n",
    "print(df_train.head())\n",
    "\n",
    "# 过滤掉过长的样本\n",
    "MAX_LENGTH = 32\n",
    "df_train = df_train[~(df_train.title1_zh.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "df_train = df_train[~(df_train.title2_zh.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "\n",
    "# 只使用部分数据集进行训练\n",
    "SAMPLE_FRAC = 0.1\n",
    "df_train = df_train.sample(frac=SAMPLE_FRAC, random_state=9527)\n",
    "\n",
    "# 提取有用的列，去除不必要的列\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.loc[:, ['title1_zh', 'title2_zh', 'label']]\n",
    "df_train.columns = ['text_a', 'text_b', 'label']\n",
    "\n",
    "# 二次分割为训练集和验证集\n",
    "df_total = df_train\n",
    "df_train = df_total.sample(frac=0.8, random_state=9527)\n",
    "df_valid = df_total[~df_total.index.isin(df_train.index)]\n",
    "\n",
    "print(\"训练样本数： \", len(df_train))\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"验证样本数： \", len(df_valid))\n",
    "print(df_valid.head())\n",
    "\n",
    "# 提取测试集数据\n",
    "test_set_name = \"FakeNewsTest\"\n",
    "test_set = azureml.core.Dataset.get_by_name(workspace=ws, name=test_set_name)\n",
    "df_test = test_set.to_pandas_dataframe()\n",
    "df_test = df_test.loc[:, [\"title1_zh\", \"title2_zh\", \"id\"]]\n",
    "df_test.columns = [\"text_a\", \"text_b\", \"Id\"]\n",
    "\n",
    "print(\"预测样本数： \", len(df_test))\n",
    "print(df_test.head())\n",
    "\n",
    "################################################################\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    # 初始化数据集\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"valid\", \"test\"]\n",
    "        self.mode = mode\n",
    "        if self.mode == \"train\":\n",
    "            self.df = df_train\n",
    "        elif self.mode == \"valid\":\n",
    "            self.df = df_valid\n",
    "        else:\n",
    "            self.df = df_test\n",
    "        \n",
    "        self.df = self.df.dropna()\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    # 获得数据集中的数据\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_a, text_b = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # 第一个句子的tokens\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 第二个句子的tokens\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # 将tokens转换成token_ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # segments_tensor用于区分两个句子\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 训练集/数据集，有无label？\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero padding\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "    # masks tensors\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "################################################################\n",
    "\n",
    "# 初始化训练集和验证集\n",
    "BATCH_SIZE = 64\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "trainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "validset = FakeNewsDataset(\"valid\", tokenizer=tokenizer)\n",
    "validloader = DataLoader(validset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "\n",
    "# 载入预训练的BERT模型\n",
    "NUM_LABELS = 3\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "# 将模型转换到GPU上\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "# 模型结构\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍历整个数据集\n",
    "        for data in dataloader:\n",
    "            # 将数据传送给GPU进行运算\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 使用数据进行预测\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors)\n",
    "            \n",
    "            # 获取预测结果\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 记录分类准确率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 记录预测结果\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# 将模型转换到GPU上\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad] \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "\n",
    "print(model.config)\n",
    "print(f\"\"\"\n",
    "分类模型的参数：{sum(p.numel() for p in model_params)}\n",
    "线性分类器的参数：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")\n",
    "\n",
    "################################################################\n",
    "\n",
    "# 训练模型（微调）\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam 自适应优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "run = experiment.start_logging()\n",
    "EPOCHS = 6\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, \n",
    "                   attention_mask=masks_tensors, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累加当前Loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # 计算和统计模型数据\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    _, v_acc = get_predictions(model, validloader, compute_acc=True)\n",
    "    print('[Epoch %d] loss: %.3f, acc: %.3f, valid acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc, v_acc))\n",
    "    run.log(\"Loss\", running_loss)\n",
    "    run.log(\"Accuracy\", acc)\n",
    "    run.log(\"Validation accuracy\", v_acc)\n",
    "    \n",
    "    # 保存模型\n",
    "    model_name = \"model_\" + str(epoch+1) + \".pkl\"\n",
    "    filename = \"outputs/\" +  model_name\n",
    "    torch.save(model.state_dict(), filename)\n",
    "run.complete()\n",
    "\n",
    "################################################################\n",
    "\n",
    "# 测试模型\n",
    "testset = FakeNewsDataset(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=128, collate_fn=create_mini_batch)\n",
    "\n",
    "predictions = get_predictions(model, testloader)\n",
    "index_map = {v: k for k, v in testset.label_map.items()}\n",
    "\n",
    "df = pd.DataFrame({\"Category\": predictions.tolist()})\n",
    "df['Category'] = df.Category.apply(lambda x: index_map[x])\n",
    "df_pred = pd.concat([testset.df.loc[:, [\"Id\"]], df.loc[:, 'Category']], axis=1)\n",
    "df_pred.to_csv('outputs/bert_prec_training_samples.csv', index=False)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.3.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "script_params = {\n",
    "    # to mount files referenced by mnist dataset\n",
    "    '--regularization': 0.5\n",
    "}\n",
    "\n",
    "estimator = PyTorch(source_directory=script_folder,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='train.py',\n",
    "                    node_count=1,\n",
    "                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>FakeNewsDetectSmall</td><td>FakeNewsDetectSmall_1575446831_f59c9f3b</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/FakeNewsDetectSmall/runs/FakeNewsDetectSmall_1575446831_f59c9f3b?wsid=/subscriptions/6e12bb22-e64d-4e66-b7ad-a5ef94af1799/resourcegroups/azureml/workspaces/azure-ml-test\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: FakeNewsDetectSmall,\n",
       "Id: FakeNewsDetectSmall_1575446831_f59c9f3b,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = experiment.submit(config=estimator)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
